{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e8151f-de20-42da-9016-fd60168b8258",
   "metadata": {},
   "source": [
    "# UNDER DEVELOPMENT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from y0.examples import *\n",
    "import sympytorch\n",
    "import sympy\n",
    "from sympy import solve, Eq\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9c8f0-cba0-43f7-a069-de7cfa44c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frontdoor_example.graph.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea6fed-ea0f-4bd2-82c7-af807543de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frontdoor_example.graph.to_linear_scm_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae27ba-34ec-446e-9bab-18fc75432173",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = frontdoor_example.graph.to_linear_scm_sympy()\n",
    "sp[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af5cd8-5b91-4581-96ac-0e83b98dcc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names, expressions = zip(*sorted(sp.items()))\n",
    "names, expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cdead2-2162-4d1f-9575-9b1f1ab8fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "epsilon_x = torch.rand(n)\n",
    "epsilon_y = torch.rand(n)\n",
    "epsilon_z = torch.rand(n)\n",
    "gamma_xy = torch.rand(n)\n",
    "beta_zy = torch.rand(n)\n",
    "beta_xz = torch.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bedae93-176c-4652-b170-40c5dc5505c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_params = {\n",
    "    r\"\\epsilon_{X}\": epsilon_x,\n",
    "    r\"\\epsilon_{Y}\": epsilon_y,\n",
    "    r\"\\epsilon_{Z}\": epsilon_z,\n",
    "    r\"\\gamma_{X,Y}\": gamma_xy,\n",
    "    r\"\\beta_{Z,Y}\": beta_zy,\n",
    "    r\"\\beta_{X,Z}\": beta_xz,\n",
    "}\n",
    "params = {\n",
    "    name: value.detach().cpu().numpy().squeeze()\n",
    "    for name, value in torch_params.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4dcea-cc6e-48bf-88fe-f13d4c825b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(3).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f8e4f-103d-4a28-9a90-db3739441fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eqns = [Eq(lhs.to_sympy(), rhs.subs(params)) for lhs, rhs in sp.items()]\n",
    "eqns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e59239-0ee4-4815-92cc-7f0cbf24bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = solve(eqns, [n.to_sympy() for n in names], rational=False)\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95724d13-d869-41ce-b3e3-567303f9b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "[type(float(v)) for v in solution.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ceaeab-97ee-4691-98ad-d983ad37d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_vars = {str(k): torch.tensor(float(v)) for k, v in solution.items()}\n",
    "torch_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f4f13-1396-4814-ab9d-fad6d778f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sympytorch.SymPyModule(expressions=expressions)\n",
    "\n",
    "scm_torch_vars = mod(**torch_params, **torch_vars)\n",
    "scm_torch_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19315c3e-617c-4b04-9c10-c1f57e5065b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81330fd0-0676-4e64-9cfb-e18a89816b0e",
   "metadata": {},
   "source": [
    "## Something\n",
    "\n",
    "Z*\\beta_{Z,Y} + \\epsilon_{Y} + \\gamma_{X,Y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b60cd-24c9-4ffc-8deb-5b1679b8ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    scm_torch_vars[0][1]\n",
    "    == torch_vars[\"Z\"] * torch_params[r\"\\beta_{Z,Y}\"]\n",
    "    + torch_params[r\"\\epsilon_{Y}\"]\n",
    "    + torch_params[r\"\\gamma_{X,Y}\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add0f3e-90c8-44d4-b0ee-2ba46771b84e",
   "metadata": {},
   "source": [
    "X*\\beta_{X,Z} + \\epsilon_{Z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639862-c2bf-4bff-92fb-da71ec3e910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    scm_torch_vars[0][2]\n",
    "    == torch_vars[\"X\"] * torch_params[r\"\\beta_{X,Z}\"] + torch_params[r\"\\epsilon_{Z}\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325d6d1-afe0-4053-99f3-55835eae6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert scm_torch_vars[0][0] == torch_params[\"\\\\epsilon_{X}\"] + torch_params[\"\\\\gamma_{X,Y}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed79e9-ec40-454f-96ee-a382bdf35bf7",
   "metadata": {},
   "source": [
    "# Pyro single door "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9b6ed-75cc-4f0e-a543-9ea00b522750",
   "metadata": {},
   "source": [
    "In Pyro, you would often use a probabilistic programming approach to define your model using distributions for both data and parameters. The objective is usually to infer the values of the learnable parameters that explain the observed data. When translating the provided deterministic function to a Pyro model, it requires a bit of reinterpretation since Pyro operates on a different paradigm. Below is a Pyro translation that adheres to the spirit of your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734df103-e6f6-4e9b-95d6-f1bfba13e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyro\n",
    "# import pyro.distributions as dist\n",
    "# import torch\n",
    "# from pyro.infer import SVI, Trace_ELBO\n",
    "# from pyro.optim import Adam\n",
    "# import pandas as pd\n",
    "# from networkx import NetworkXNotImplemented\n",
    "# from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "\n",
    "\n",
    "# def pyro_single_door_learnable(graph, data):\n",
    "#     \"\"\"\n",
    "#     Pyro model to estimate parameter values for a linear SCM using backdoor adjustment.\n",
    "#     Observe data as noisy observations of state variables, which are functions of learnable parameters.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Convert the pandas dataframe to a tensor for use with Pyro\n",
    "#     data_tensor = torch.tensor(data.values, dtype=torch.float32)\n",
    "#     num_obs, num_vars = data_tensor.shape\n",
    "\n",
    "#     # Define a guide for our parameters (here, an AutoDiagonalNormal -- a form of automatic variational inference)\n",
    "#     # We'll have as many latent variables as there are edges in the graph\n",
    "#     num_params = len(graph.directed.edges)\n",
    "#     guide = AutoDiagonalNormal(lambda: pyro.param(\"coefs\", torch.zeros(num_params)))\n",
    "\n",
    "#     # Prepare a dictionary to collect parameter values\n",
    "#     estimated_params = {}\n",
    "\n",
    "#     # The model\n",
    "#     def model():\n",
    "#         # Iterate over each edge in the graph as before\n",
    "#         priors = {node: pyro.sample(f\"prior_{str(node)}\", dist.Normal(0.0, 1.0)) for node in graph}\n",
    "\n",
    "#         for source, target in graph.directed.edges():\n",
    "\n",
    "#             # Check for NetworkXNotImplemented error to adhere to pyro's exception handling\n",
    "#             try:\n",
    "#                 adjustment_sets = (\n",
    "#                     graph.to_pgmpy_causal_inference().get_all_backdoor_adjustment_sets(\n",
    "#                         source.name, target.name\n",
    "#                     )\n",
    "#                 )\n",
    "#             except ValueError or NetworkXNotImplemented:\n",
    "#                 continue  # Skip if no valid backdoor adjustment sets\n",
    "\n",
    "#             if not adjustment_sets:\n",
    "#                 continue  # Skip if no valid backdoor adjustment sets\n",
    "\n",
    "#             adjustment_set = list(adjustment_sets)[0]\n",
    "#             variables = sorted(adjustment_set | {source.name})\n",
    "#             idx = variables.index(source.name)\n",
    "\n",
    "#             # Define a learnable parameter for the coefficient of the source variable\n",
    "#             coef = pyro.param(f\"coef_{source.name}_to_{target.name}\", torch.tensor(0.0))\n",
    "#             # Specify priors on other variables\n",
    "\n",
    "#             # Observations based on the linear combination of variables\n",
    "#             mean = coef * data_tensor[:, idx] + torch.tensor(priors).sum(axis=0)\n",
    "#             with pyro.plate(f\"data_plate_{source.name}_{target.name}\", size=num_obs, dim=-1):\n",
    "#                 # Observe the data\n",
    "#                 pyro.sample(\n",
    "#                     f\"obs_{source.name}_{target.name}\",\n",
    "#                     dist.Normal(mean, 1.0),  # Assuming noise of 1.0 for simplicity\n",
    "#                     obs=data_tensor[:, data.columns.get_loc(target.name)],\n",
    "#                 )\n",
    "\n",
    "#             # Keep estimated parameters in a dictionary\n",
    "#             estimated_params[(source, target)] = coef.item()\n",
    "\n",
    "#     # Setup optimization\n",
    "#     adam = Adam({\"lr\": 0.01})\n",
    "#     svi = SVI(model=model, guide=guide, optim=adam, loss=Trace_ELBO())\n",
    "\n",
    "#     # Run optimization/learning\n",
    "#     num_iterations = 1000  # Set an appropriate number of iterations\n",
    "#     for _ in range(num_iterations):\n",
    "#         svi.step()\n",
    "\n",
    "#     # Return the learned coefficients\n",
    "#     return estimated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2f5bb-5881-4553-af56-655ffebcfb22",
   "metadata": {},
   "source": [
    "In this translation:\n",
    "1. We define a Pyro model that observes data and estimates the coefficient for the `source` variable that corresponds to the linear regression coefficient.\n",
    "2. We parametrize the coefficients for each edge in the graph and use Pyro's SVI (Stochastic Variational Inference) for optimization.\n",
    "3. An `AutoDiagonalNormal` guide is used, which is a simple but often effective choice for variational inference with continuous latent variables.\n",
    "4. Pyro plates are used to handle vectorized observations for the target variable.\n",
    "5. The learned parameters are stored in a dictionary, similar to the original specification.\n",
    "\n",
    "Note that in a real application, you would need to choose appropriate priors based on your domain knowledge and likely need to adjust aspects of the guide and inference for the model to fit well. This translation is not exact but provides a structure to build upon using Pyro's methodology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
